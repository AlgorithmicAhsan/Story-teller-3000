{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30509b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial vocab size: 65\n",
      "Final vocab size: 250\n",
      "Tokenizer saved.\n",
      "Tokenizer loaded.\n",
      "\n",
      "Original: ایک دفعہ کا ذکر ہے۔␞\n",
      "Tokens: [118, 23, 35, 33, 72, 54, 69, 24, 54, 68, 58, 60, 61, 64]\n",
      "Decoded: ایک دفعہ کا ذکر ہے۔␞\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "# =================================================\n",
    "# Special Tokens (Never Merged)\n",
    "# =================================================\n",
    "\n",
    "EOS = \"\\u241E\"   # ␞\n",
    "EOP = \"\\u241D\"   # ␝\n",
    "EOT = \"\\u0003\"   # End of Text\n",
    "\n",
    "SPECIAL_TOKENS = [EOS, EOP, EOT]\n",
    "\n",
    "\n",
    "# =================================================\n",
    "# Train BPE\n",
    "# =================================================\n",
    "\n",
    "def train_bpe(texts, max_vocab_size=250):\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # Build Initial Character Vocab\n",
    "    # ---------------------------------------------\n",
    "\n",
    "    chars = set(\"\".join(texts))\n",
    "\n",
    "    for t in SPECIAL_TOKENS:\n",
    "        chars.add(t)\n",
    "\n",
    "    chars = sorted(chars)\n",
    "\n",
    "    char2id = {c: i for i, c in enumerate(chars)}\n",
    "    id2char = {i: c for c, i in char2id.items()}\n",
    "\n",
    "\n",
    "    # Convert texts to ID sequences\n",
    "    sequences = [\n",
    "        [char2id[c] for c in text]\n",
    "        for text in texts\n",
    "    ]\n",
    "\n",
    "\n",
    "    merges = {}        # new_id -> (a,b)\n",
    "    next_id = len(char2id)\n",
    "\n",
    "    special_ids = {char2id[t] for t in SPECIAL_TOKENS}\n",
    "\n",
    "\n",
    "    print(\"Initial vocab size:\", next_id)\n",
    "\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # BPE Loop\n",
    "    # ---------------------------------------------\n",
    "\n",
    "    while next_id < max_vocab_size:\n",
    "\n",
    "        pair_counts = {}\n",
    "\n",
    "\n",
    "        # Count pairs\n",
    "        for seq in sequences:\n",
    "\n",
    "            for pair in zip(seq, seq[1:]):\n",
    "\n",
    "                # Skip special tokens\n",
    "                if pair[0] in special_ids or pair[1] in special_ids:\n",
    "                    continue\n",
    "\n",
    "                pair_counts[pair] = pair_counts.get(pair, 0) + 1\n",
    "\n",
    "\n",
    "        if not pair_counts:\n",
    "            break\n",
    "\n",
    "\n",
    "        # Most frequent pair\n",
    "        best_pair = max(pair_counts, key=pair_counts.get)\n",
    "\n",
    "        new_id = next_id\n",
    "        next_id += 1\n",
    "\n",
    "        merges[new_id] = best_pair\n",
    "\n",
    "\n",
    "        # Replace in sequences\n",
    "        new_sequences = []\n",
    "\n",
    "\n",
    "        for seq in sequences:\n",
    "\n",
    "            new_seq = []\n",
    "            i = 0\n",
    "\n",
    "            while i < len(seq):\n",
    "\n",
    "                if i + 1 < len(seq) and (seq[i], seq[i+1]) == best_pair:\n",
    "\n",
    "                    new_seq.append(new_id)\n",
    "                    i += 2\n",
    "\n",
    "                else:\n",
    "\n",
    "                    new_seq.append(seq[i])\n",
    "                    i += 1\n",
    "\n",
    "\n",
    "            new_sequences.append(new_seq)\n",
    "\n",
    "\n",
    "        sequences = new_sequences\n",
    "\n",
    "\n",
    "    print(\"Final vocab size:\", next_id)\n",
    "\n",
    "    return merges, char2id, id2char\n",
    "\n",
    "\n",
    "# =================================================\n",
    "# Save / Load\n",
    "# =================================================\n",
    "\n",
    "def save_tokenizer(merges, char2id, id2char):\n",
    "\n",
    "    # Save merges\n",
    "    with open(\"merges.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "\n",
    "        json.dump(\n",
    "            {str(k): v for k, v in merges.items()},\n",
    "            f,\n",
    "            indent=2\n",
    "        )\n",
    "\n",
    "\n",
    "    # Save vocab\n",
    "    with open(\"vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "\n",
    "        json.dump(\n",
    "            char2id,\n",
    "            f,\n",
    "            ensure_ascii=False,\n",
    "            indent=2\n",
    "        )\n",
    "\n",
    "\n",
    "    # Save reverse vocab\n",
    "    with open(\"id2char.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "\n",
    "        json.dump(\n",
    "            {str(k): v for k, v in id2char.items()},\n",
    "            f,\n",
    "            ensure_ascii=False,\n",
    "            indent=2\n",
    "        )\n",
    "\n",
    "\n",
    "    print(\"Tokenizer saved.\")\n",
    "\n",
    "\n",
    "def load_tokenizer():\n",
    "\n",
    "    # Load merges\n",
    "    with open(\"merges.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "\n",
    "        merges_raw = json.load(f)\n",
    "\n",
    "        merges = {\n",
    "            int(k): tuple(v)\n",
    "            for k, v in merges_raw.items()\n",
    "        }\n",
    "\n",
    "\n",
    "    # Load vocab\n",
    "    with open(\"vocab.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "\n",
    "        char2id = json.load(f)\n",
    "\n",
    "\n",
    "    # Load reverse vocab\n",
    "    with open(\"id2char.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "\n",
    "        raw = json.load(f)\n",
    "\n",
    "        id2char = {\n",
    "            int(k): v\n",
    "            for k, v in raw.items()\n",
    "        }\n",
    "\n",
    "\n",
    "    print(\"Tokenizer loaded.\")\n",
    "\n",
    "    return merges, char2id, id2char\n",
    "\n",
    "\n",
    "# =================================================\n",
    "# Tokenizer Class\n",
    "# =================================================\n",
    "\n",
    "class BPETokenizer:\n",
    "\n",
    "\n",
    "    def __init__(self, merges, char2id, id2char):\n",
    "\n",
    "        self.merges = merges\n",
    "        self.char2id = char2id\n",
    "        self.id2char = id2char\n",
    "\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # Encode\n",
    "    # ---------------------------------------------\n",
    "\n",
    "    def encode(self, text):\n",
    "\n",
    "        tokens = []\n",
    "\n",
    "\n",
    "        # Char → ID\n",
    "        for c in text:\n",
    "\n",
    "            if c in self.char2id:\n",
    "                tokens.append(self.char2id[c])\n",
    "            else:\n",
    "                tokens.append(self.char2id[\" \"])\n",
    "\n",
    "\n",
    "        changed = True\n",
    "\n",
    "\n",
    "        # Apply merges\n",
    "        while changed:\n",
    "\n",
    "            changed = False\n",
    "            new_tokens = []\n",
    "\n",
    "            i = 0\n",
    "\n",
    "\n",
    "            while i < len(tokens):\n",
    "\n",
    "                merged = False\n",
    "\n",
    "\n",
    "                for new_id, (a, b) in self.merges.items():\n",
    "\n",
    "                    if i + 1 < len(tokens) and tokens[i] == a and tokens[i+1] == b:\n",
    "\n",
    "                        new_tokens.append(new_id)\n",
    "\n",
    "                        i += 2\n",
    "                        merged = True\n",
    "                        changed = True\n",
    "                        break\n",
    "\n",
    "\n",
    "                if not merged:\n",
    "\n",
    "                    new_tokens.append(tokens[i])\n",
    "                    i += 1\n",
    "\n",
    "\n",
    "            tokens = new_tokens\n",
    "\n",
    "\n",
    "        return tokens\n",
    "\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # Decode\n",
    "    # ---------------------------------------------\n",
    "\n",
    "    def decode(self, tokens):\n",
    "\n",
    "\n",
    "        def expand(t):\n",
    "\n",
    "            if t in self.merges:\n",
    "\n",
    "                a, b = self.merges[t]\n",
    "                return expand(a) + expand(b)\n",
    "\n",
    "            return self.id2char[t]\n",
    "\n",
    "\n",
    "        text = \"\"\n",
    "\n",
    "        for t in tokens:\n",
    "            text += expand(t)\n",
    "\n",
    "\n",
    "        return text\n",
    "\n",
    "\n",
    "# =================================================\n",
    "# Main\n",
    "# =================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # Train (Run ONCE)\n",
    "    # ---------------------------------------------\n",
    "\n",
    "    with open(\"urdu_stories_cleaned.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "\n",
    "        data = json.load(f)\n",
    "\n",
    "    texts = [item[\"content\"] for item in data]\n",
    "\n",
    "\n",
    "    merges, char2id, id2char = train_bpe(texts, 250)\n",
    "\n",
    "    save_tokenizer(merges, char2id, id2char)\n",
    "\n",
    "\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # Load (Run EVERY TIME)\n",
    "    # ---------------------------------------------\n",
    "\n",
    "    merges, char2id, id2char = load_tokenizer()\n",
    "\n",
    "    tokenizer = BPETokenizer(merges, char2id, id2char)\n",
    "\n",
    "\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # Test\n",
    "    # ---------------------------------------------\n",
    "\n",
    "    example = \"ایک دفعہ کا ذکر ہے۔\" + EOS\n",
    "\n",
    "    encoded = tokenizer.encode(example)\n",
    "    decoded = tokenizer.decode(encoded)\n",
    "\n",
    "\n",
    "    print(\"\\nOriginal:\", example)\n",
    "    print(\"Tokens:\", encoded)\n",
    "    print(\"Decoded:\", decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10714d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded.\n",
      "Tokenizing 200 stories...\n",
      "  Tokenized 50/200 stories...\n",
      "  Tokenized 100/200 stories...\n",
      "  Tokenized 150/200 stories...\n",
      "  Tokenized 200/200 stories...\n",
      "\n",
      "Saved tokenized corpus to 'tokenized_corpus.json'\n",
      "Total stories: 200\n",
      "\n",
      "Sample tokenized story:\n",
      "  Title: Billi Sher Ki Khala Hai - Article No. 2920\n",
      "  Tokens (first 20): [28, 59, 68, 20, 210, 101, 125, 71, 12, 149, 90, 54, 68, 228, 78, 69, 54, 72, 212, 17]\n",
      "  Total tokens: 1314\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "merges, char2id, id2char = load_tokenizer()\n",
    "tokenizer = BPETokenizer(merges, char2id, id2char)\n",
    "\n",
    "# Load stories\n",
    "with open(\"urdu_stories_cleaned.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    stories = json.load(f)\n",
    "\n",
    "print(f\"Tokenizing {len(stories)} stories...\")\n",
    "\n",
    "# Tokenize all stories\n",
    "tokenized_corpus = []\n",
    "for i, story in enumerate(stories):\n",
    "    token_ids = tokenizer.encode(story[\"content\"])\n",
    "    tokenized_corpus.append({\n",
    "        \"title\": story[\"title\"],\n",
    "        \"url\": story[\"url\"],\n",
    "        \"tokens\": token_ids,\n",
    "        \"num_tokens\": len(token_ids)\n",
    "    })\n",
    "    \n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"  Tokenized {i + 1}/{len(stories)} stories...\")\n",
    "\n",
    "# Save tokenized corpus\n",
    "with open(\"tokenized_corpus.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(tokenized_corpus, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved tokenized corpus to 'tokenized_corpus.json'\")\n",
    "print(f\"Total stories: {len(tokenized_corpus)}\")\n",
    "print(f\"\\nSample tokenized story:\")\n",
    "print(f\"  Title: {tokenized_corpus[0]['title']}\")\n",
    "print(f\"  Tokens (first 20): {tokenized_corpus[0]['tokens'][:20]}\")\n",
    "print(f\"  Total tokens: {tokenized_corpus[0]['num_tokens']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
