{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30509b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial vocab size: 69\n",
      "Final vocab size: 250\n",
      "Tokenizer saved.\n",
      "Tokenizer loaded.\n",
      "\n",
      "Original: ایک دفعہ کا ذکر ہے۔␞\n",
      "Tokens: [121, 26, 38, 36, 61, 124, 27, 57, 72, 215, 64, 67]\n",
      "Decoded: ایک دفعہ کا ذکر ہے۔␞\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# =================================================\n",
    "# Special Tokens (Never Merged)\n",
    "# =================================================\n",
    "\n",
    "EOS = \"\\u241E\"   # ␞\n",
    "EOP = \"\\u241D\"   # ␝\n",
    "EOT = \"\\u0003\"   # End of Text\n",
    "\n",
    "SPECIAL_TOKENS = [EOS, EOP, EOT]\n",
    "\n",
    "\n",
    "# =================================================\n",
    "# Train BPE\n",
    "# =================================================\n",
    "\n",
    "def train_bpe(texts, max_vocab_size=250):\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # Initial Character Vocabulary\n",
    "    # ---------------------------------------------\n",
    "\n",
    "    chars = set(\"\".join(texts))\n",
    "    for t in SPECIAL_TOKENS:\n",
    "        chars.add(t)\n",
    "\n",
    "    chars = sorted(chars)\n",
    "\n",
    "    char2id = {c: i for i, c in enumerate(chars)}\n",
    "    id2char = {i: c for c, i in char2id.items()}\n",
    "\n",
    "    sequences = [\n",
    "        [char2id[c] for c in text]\n",
    "        for text in texts\n",
    "    ]\n",
    "\n",
    "    merges = []  # ordered list of (new_id, (a,b))\n",
    "    next_id = len(char2id)\n",
    "\n",
    "    special_ids = {char2id[t] for t in SPECIAL_TOKENS}\n",
    "\n",
    "    print(\"Initial vocab size:\", next_id)\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # BPE Loop\n",
    "    # ---------------------------------------------\n",
    "\n",
    "    while next_id < max_vocab_size:\n",
    "\n",
    "        pair_counts = {}\n",
    "\n",
    "        for seq in sequences:\n",
    "            for pair in zip(seq, seq[1:]):\n",
    "                if pair[0] in special_ids or pair[1] in special_ids:\n",
    "                    continue\n",
    "                pair_counts[pair] = pair_counts.get(pair, 0) + 1\n",
    "\n",
    "        if not pair_counts:\n",
    "            break\n",
    "\n",
    "        best_pair = max(pair_counts, key=pair_counts.get)\n",
    "\n",
    "        new_id = next_id\n",
    "        next_id += 1\n",
    "\n",
    "        a, b = best_pair\n",
    "\n",
    "        # Create token string\n",
    "        token_str = id2char[a] + id2char[b]\n",
    "\n",
    "        # Update vocab\n",
    "        char2id[token_str] = new_id\n",
    "        id2char[new_id] = token_str\n",
    "\n",
    "        # Store merge in order\n",
    "        merges.append((new_id, best_pair))\n",
    "\n",
    "        # Replace in sequences\n",
    "        new_sequences = []\n",
    "\n",
    "        for seq in sequences:\n",
    "            new_seq = []\n",
    "            i = 0\n",
    "\n",
    "            while i < len(seq):\n",
    "                if i + 1 < len(seq) and (seq[i], seq[i+1]) == best_pair:\n",
    "                    new_seq.append(new_id)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_seq.append(seq[i])\n",
    "                    i += 1\n",
    "\n",
    "            new_sequences.append(new_seq)\n",
    "\n",
    "        sequences = new_sequences\n",
    "\n",
    "    print(\"Final vocab size:\", next_id)\n",
    "\n",
    "    return merges, char2id, id2char\n",
    "\n",
    "\n",
    "# =================================================\n",
    "# Save / Load\n",
    "# =================================================\n",
    "\n",
    "def save_tokenizer(merges, char2id, id2char):\n",
    "\n",
    "    # Save merges as ordered list\n",
    "    with open(\"merges.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(\n",
    "            [(new_id, list(pair)) for new_id, pair in merges],\n",
    "            f,\n",
    "            indent=2\n",
    "        )\n",
    "\n",
    "    with open(\"vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(char2id, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    with open(\"id2char.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(\n",
    "            {str(k): v for k, v in id2char.items()},\n",
    "            f,\n",
    "            ensure_ascii=False,\n",
    "            indent=2\n",
    "        )\n",
    "\n",
    "    print(\"Tokenizer saved.\")\n",
    "\n",
    "\n",
    "def load_tokenizer():\n",
    "\n",
    "    with open(\"merges.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        merges_raw = json.load(f)\n",
    "        merges = [\n",
    "            (int(new_id), tuple(pair))\n",
    "            for new_id, pair in merges_raw\n",
    "        ]\n",
    "\n",
    "    with open(\"vocab.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        char2id = json.load(f)\n",
    "\n",
    "    with open(\"id2char.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = json.load(f)\n",
    "        id2char = {int(k): v for k, v in raw.items()}\n",
    "\n",
    "    print(\"Tokenizer loaded.\")\n",
    "\n",
    "    return merges, char2id, id2char\n",
    "\n",
    "\n",
    "# =================================================\n",
    "# Tokenizer Class\n",
    "# =================================================\n",
    "\n",
    "class BPETokenizer:\n",
    "\n",
    "    def __init__(self, merges, char2id, id2char):\n",
    "        self.merges = merges\n",
    "        self.char2id = char2id\n",
    "        self.id2char = id2char\n",
    "        self.merge_dict = {new_id: pair for new_id, pair in merges}\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # Encode (Oldest → Newest)\n",
    "    # ---------------------------------------------\n",
    "\n",
    "    def encode(self, text):\n",
    "\n",
    "        tokens = []\n",
    "\n",
    "        for c in text:\n",
    "            if c in self.char2id:\n",
    "                tokens.append(self.char2id[c])\n",
    "            else:\n",
    "                tokens.append(self.char2id.get(\" \", 0))\n",
    "\n",
    "        # Apply merges in order\n",
    "        for new_id, (a, b) in self.merges:\n",
    "\n",
    "            new_tokens = []\n",
    "            i = 0\n",
    "\n",
    "            while i < len(tokens):\n",
    "                if i + 1 < len(tokens) and tokens[i] == a and tokens[i+1] == b:\n",
    "                    new_tokens.append(new_id)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_tokens.append(tokens[i])\n",
    "                    i += 1\n",
    "\n",
    "            tokens = new_tokens\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # Decode (Newest → Oldest via recursion)\n",
    "    # ---------------------------------------------\n",
    "\n",
    "    def decode(self, tokens):\n",
    "\n",
    "        def expand(t):\n",
    "            if t in self.merge_dict:\n",
    "                a, b = self.merge_dict[t]\n",
    "                return expand(a) + expand(b)\n",
    "            return self.id2char[t]\n",
    "\n",
    "        return \"\".join(expand(t) for t in tokens)\n",
    "\n",
    "\n",
    "# =================================================\n",
    "# Main\n",
    "# =================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    with open(\"urdu_stories_cleaned.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    texts = [item[\"content\"] for item in data]\n",
    "\n",
    "    merges, char2id, id2char = train_bpe(texts, 250)\n",
    "\n",
    "    save_tokenizer(merges, char2id, id2char)\n",
    "\n",
    "    merges, char2id, id2char = load_tokenizer()\n",
    "\n",
    "    tokenizer = BPETokenizer(merges, char2id, id2char)\n",
    "\n",
    "    example = \"ایک دفعہ کا ذکر ہے۔\" + EOS\n",
    "\n",
    "    encoded = tokenizer.encode(example)\n",
    "    decoded = tokenizer.decode(encoded)\n",
    "\n",
    "    print(\"\\nOriginal:\", example)\n",
    "    print(\"Tokens:\", encoded)\n",
    "    print(\"Decoded:\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10714d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded.\n",
      "Tokenizing 500 stories...\n",
      "  Tokenized 50/500 stories...\n",
      "  Tokenized 100/500 stories...\n",
      "  Tokenized 150/500 stories...\n",
      "  Tokenized 200/500 stories...\n",
      "  Tokenized 250/500 stories...\n",
      "  Tokenized 300/500 stories...\n",
      "  Tokenized 350/500 stories...\n",
      "  Tokenized 400/500 stories...\n",
      "  Tokenized 450/500 stories...\n",
      "  Tokenized 500/500 stories...\n",
      "\n",
      "Saved tokenized corpus to 'tokenized_corpus.json'\n",
      "Total stories: 500\n",
      "\n",
      "Sample tokenized story:\n",
      "  Title: Allah Ka Dost - Article No. 2923\n",
      "  Tokens (first 20): [84, 121, 82, 195, 107, 90, 75, 91, 129, 28, 73, 41, 33, 41, 43, 103, 82, 40, 137, 116]\n",
      "  Total tokens: 2085\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "merges, char2id, id2char = load_tokenizer()\n",
    "tokenizer = BPETokenizer(merges, char2id, id2char)\n",
    "\n",
    "# Load stories\n",
    "with open(\"urdu_stories_cleaned.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    stories = json.load(f)\n",
    "\n",
    "print(f\"Tokenizing {len(stories)} stories...\")\n",
    "\n",
    "# Tokenize all stories\n",
    "tokenized_corpus = []\n",
    "for i, story in enumerate(stories):\n",
    "    token_ids = tokenizer.encode(story[\"content\"])\n",
    "    tokenized_corpus.append({\n",
    "        \"title\": story[\"title\"],\n",
    "        \"url\": story[\"url\"],\n",
    "        \"tokens\": token_ids,\n",
    "        \"num_tokens\": len(token_ids)\n",
    "    })\n",
    "    \n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"  Tokenized {i + 1}/{len(stories)} stories...\")\n",
    "\n",
    "# Save tokenized corpus\n",
    "with open(\"tokenized_corpus.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(tokenized_corpus, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved tokenized corpus to 'tokenized_corpus.json'\")\n",
    "print(f\"Total stories: {len(tokenized_corpus)}\")\n",
    "print(f\"\\nSample tokenized story:\")\n",
    "print(f\"  Title: {tokenized_corpus[0]['title']}\")\n",
    "print(f\"  Tokens (first 20): {tokenized_corpus[0]['tokens'][:20]}\")\n",
    "print(f\"  Total tokens: {tokenized_corpus[0]['num_tokens']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
