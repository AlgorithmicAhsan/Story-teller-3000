{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64bf6625",
   "metadata": {},
   "source": [
    "# Trigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0502aa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "import sys\n",
    "sys.path.append(\"../tokenizer\")\n",
    "from bpe_tokenizer import get_tokenizer, load_tokenizer, EOT, EOS, EOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30f83f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200 stories, Vocab size: 65, EOT_ID: 0\n"
     ]
    }
   ],
   "source": [
    "with open(\"../tokenizer/tokenized_corpus.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    corpus = json.load(f)\n",
    "\n",
    "merges, char2id, id2char = load_tokenizer(\"../tokenizer\")\n",
    "\n",
    "EOT_ID = char2id[EOT]\n",
    "VOCAB_SIZE = len(char2id)\n",
    "\n",
    "tokenized_stories = [story[\"tokens\"] for story in corpus]\n",
    "\n",
    "print(f\"Loaded {len(tokenized_stories)} stories, Vocab size: {VOCAB_SIZE}, EOT_ID: {EOT_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b835af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigram Language Model with Interpolation and Perplexity Calculation\n",
    "class TrigramLanguageModel:\n",
    "    def __init__(self, vocab_size):\n",
    "        self.unigrams = Counter()\n",
    "        self.bigrams = Counter()\n",
    "        self.trigrams = Counter()\n",
    "        self.total_tokens = 0\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def train(self, tokenized_stories):\n",
    "        for story in tokenized_stories:\n",
    "            self.total_tokens += len(story)\n",
    "\n",
    "            for i in range(len(story)):\n",
    "                self.unigrams[story[i]] += 1\n",
    "\n",
    "                if i >= 1:\n",
    "                    self.bigrams[(story[i-1], story[i])] += 1\n",
    "\n",
    "                if i >= 2:\n",
    "                    self.trigrams[(story[i-2], story[i-1], story[i])] += 1\n",
    "\n",
    "    def unigram_prob(self, w):\n",
    "        return (self.unigrams[w] + 1) / (self.total_tokens + self.vocab_size)\n",
    "\n",
    "    def bigram_prob(self, w1, w2):\n",
    "        denom = self.unigrams[w1] + self.vocab_size\n",
    "        return (self.bigrams[(w1, w2)] + 1) / denom\n",
    "\n",
    "    def trigram_prob(self, w1, w2, w3):\n",
    "        denom = self.bigrams[(w1, w2)] + self.vocab_size\n",
    "        return (self.trigrams[(w1, w2, w3)] + 1) / denom\n",
    "\n",
    "    def interpolated_prob(self, w1, w2, w3, l1, l2, l3):\n",
    "        p1 = self.unigram_prob(w3)\n",
    "        p2 = self.bigram_prob(w2, w3)\n",
    "        p3 = self.trigram_prob(w1, w2, w3)\n",
    "        return l1 * p1 + l2 * p2 + l3 * p3\n",
    "\n",
    "    def perplexity(self, tokenized_stories, l1, l2, l3):\n",
    "        log_prob_sum = 0\n",
    "        N = 0\n",
    "\n",
    "        for story in tokenized_stories:\n",
    "            for i in range(2, len(story)):\n",
    "                w1, w2, w3 = story[i-2], story[i-1], story[i]\n",
    "                prob = self.interpolated_prob(w1, w2, w3, l1, l2, l3)\n",
    "\n",
    "                if prob > 0:\n",
    "                    log_prob_sum += math.log(prob)\n",
    "                else:\n",
    "                    log_prob_sum += math.log(1e-10)\n",
    "\n",
    "                N += 1\n",
    "\n",
    "        return math.exp(-log_prob_sum / N)\n",
    "\n",
    "    def generate(self, prefix_tokens, max_length, l1, l2, l3, eot_id, temperature=0.5):\n",
    "        tokens = list(prefix_tokens)\n",
    "\n",
    "        while len(tokens) < 2:\n",
    "            tokens.insert(0, tokens[0] if tokens else 0)\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            w1, w2 = tokens[-2], tokens[-1]\n",
    "\n",
    "            probs = []\n",
    "            for token_id in range(self.vocab_size):\n",
    "                p = self.interpolated_prob(w1, w2, token_id, l1, l2, l3)\n",
    "                probs.append(p)\n",
    "\n",
    "            probs = [p ** (1 / temperature) for p in probs]\n",
    "            total = sum(probs)\n",
    "            probs = [p / total for p in probs]\n",
    "\n",
    "            next_token = random.choices(range(self.vocab_size), weights=probs, k=1)[0]\n",
    "            tokens.append(next_token)\n",
    "\n",
    "            if next_token == eot_id:\n",
    "                break\n",
    "\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae4a0158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_lambdas(model, dev_data):\n",
    "    best_perplexity = float(\"inf\")\n",
    "    best_lambdas = (0, 0, 0)\n",
    "\n",
    "    for l1 in [0.1, 0.2, 0.3]:\n",
    "        for l2 in [0.1, 0.2, 0.3]:\n",
    "            l3 = 1 - l1 - l2\n",
    "            if l3 <= 0:\n",
    "                continue\n",
    "\n",
    "            perp = model.perplexity(dev_data, l1, l2, l3)\n",
    "\n",
    "            if perp < best_perplexity:\n",
    "                best_perplexity = perp\n",
    "                best_lambdas = (l1, l2, l3)\n",
    "\n",
    "    return best_lambdas\n",
    "\n",
    "\n",
    "def split_data(tokenized_stories):\n",
    "    shuffled = tokenized_stories.copy()\n",
    "    random.shuffle(shuffled)\n",
    "    n = len(shuffled)\n",
    "\n",
    "    train = shuffled[:int(0.7*n)]\n",
    "    dev   = shuffled[int(0.7*n):int(0.8*n)]\n",
    "    test  = shuffled[int(0.8*n):]\n",
    "\n",
    "    return train, dev, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcd0b5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 140, Dev: 20, Test: 40\n",
      "Total tokens: 237,062\n"
     ]
    }
   ],
   "source": [
    "train_data, dev_data, test_data = split_data(tokenized_stories)\n",
    "\n",
    "model = TrigramLanguageModel(VOCAB_SIZE)\n",
    "model.train(train_data)\n",
    "\n",
    "print(f\"Train: {len(train_data)}, Dev: {len(dev_data)}, Test: {len(test_data)}\")\n",
    "print(f\"Total tokens: {model.total_tokens:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26bd6f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best lambdas: l1=0.1, l2=0.2, l3=0.7\n",
      "Test Perplexity: 20.60\n"
     ]
    }
   ],
   "source": [
    "l1, l2, l3 = tune_lambdas(model, dev_data)\n",
    "print(f\"Best lambdas: l1={l1}, l2={l2}, l3={l3}\")\n",
    "\n",
    "test_perplexity = model.perplexity(test_data, l1, l2, l3)\n",
    "print(f\"Test Perplexity: {test_perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e03eb250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model trained on 336,417 tokens\n"
     ]
    }
   ],
   "source": [
    "final_model = TrigramLanguageModel(VOCAB_SIZE)\n",
    "final_model.train(tokenized_stories)\n",
    "print(f\"Final model trained on {final_model.total_tokens:,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "184d7893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to trigram_model.json\n"
     ]
    }
   ],
   "source": [
    "model_data = {\n",
    "    \"unigrams\": dict(final_model.unigrams),\n",
    "    \"bigrams\": {str(k): v for k, v in final_model.bigrams.items()},\n",
    "    \"trigrams\": {str(k): v for k, v in final_model.trigrams.items()},\n",
    "    \"total_tokens\": final_model.total_tokens,\n",
    "    \"vocab_size\": final_model.vocab_size,\n",
    "    \"lambdas\": [l1, l2, l3],\n",
    "    \"eot_id\": EOT_ID\n",
    "}\n",
    "\n",
    "with open(\"trigram_model.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(model_data, f)\n",
    "\n",
    "print(\"Model saved to trigram_model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40386b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"../tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c407133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature 0.3 (very focused):\n",
      "ایک دفعہ کا ذکر ہےغرض ژکزینچ مرغ ایٹ ␝ اتفاظ دککیمسئلسلسلسلسلسیع مع معادثاقتُککعٴھح موٹ فاظ ␝ معموسط فرض ایعلق اتذلڑکایاگ ایٹ فیصلاحدکی ␝ شہزاکٹ اینچ برترکیج طاق پوٹ فہکیمپاکٹ فیصلکڑ پرنسپاکٹ آئیولڈ چکرتعلمحبکی ایچیزوڑ\n",
      "\n",
      "Temperature 0.7 (balanced):\n",
      "ایک دفعہ کا ذکر ہے\"ڑ␞ ␝ ہے'ےنکرگ␞ِھش․ \u0003\n",
      "\n",
      "Temperature 1.0 (original/random):\n",
      "ایک دفعہ کا ذکر ہےٰجےکغوک گژ․ٴتنژِ حڈ)\u0003\n"
     ]
    }
   ],
   "source": [
    "prefix = \"ایک دفعہ کا ذکر ہے\"\n",
    "prefix_tokens = tokenizer.encode(prefix)\n",
    "\n",
    "print(\"Temperature 0.3 (very focused):\")\n",
    "generated_tokens = final_model.generate(prefix_tokens, 200, l1, l2, l3, EOT_ID, temperature=0.3)\n",
    "print(tokenizer.decode(generated_tokens))\n",
    "\n",
    "print(\"\\nTemperature 0.7 (balanced):\")\n",
    "generated_tokens = final_model.generate(prefix_tokens, 200, l1, l2, l3, EOT_ID, temperature=0.7)\n",
    "print(tokenizer.decode(generated_tokens))\n",
    "\n",
    "print(\"\\nTemperature 1.0 (original/random):\")\n",
    "generated_tokens = final_model.generate(prefix_tokens, 200, l1, l2, l3, EOT_ID, temperature=1.0)\n",
    "print(tokenizer.decode(generated_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94095ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Story:\n",
      "ایک دفعہ کا ذکر ہےث␞جبرگزاہرفاکیقبکرگ ذہسط ہےمزاً\u0003\n"
     ]
    }
   ],
   "source": [
    "# loading model from the file\n",
    "with open(\"trigram_model.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    loaded = json.load(f)\n",
    "\n",
    "# Reconstruct model\n",
    "test_model = TrigramLanguageModel(loaded[\"vocab_size\"])\n",
    "test_model.unigrams = Counter(loaded[\"unigrams\"])\n",
    "test_model.bigrams = Counter({eval(k): v for k, v in loaded[\"bigrams\"].items()})\n",
    "test_model.trigrams = Counter({eval(k): v for k, v in loaded[\"trigrams\"].items()})\n",
    "test_model.total_tokens = loaded[\"total_tokens\"]\n",
    "\n",
    "l1, l2, l3 = loaded[\"lambdas\"]\n",
    "eot_id = loaded[\"eot_id\"]\n",
    "\n",
    "# Generate\n",
    "prefix = \"ایک دفعہ کا ذکر ہے\"\n",
    "prefix_tokens = tokenizer.encode(prefix)\n",
    "generated_tokens = test_model.generate(prefix_tokens, 1000, l1, l2, l3, eot_id, temperature=1.0)\n",
    "generated_text = tokenizer.decode(generated_tokens)\n",
    "\n",
    "print(\"Generated Story:\")\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
